POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=postgres
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
DB_SCHEMA=fa02_staging

KAFKA_NETWORK_NAME=fa-dae2-capstone_kafka_network

# Snowflake Configuration
SNOWFLAKE_ACCOUNT=
SNOWFLAKE_USER=
SNOWFLAKE_ROLE=
SNOWFLAKE_WAREHOUSE=
SNOWFLAKE_DATABASE=
SNOWFLAKE_SCHEMA=
# Path to the private key file (.p8 or .pem) - used by dev/prod targets
SNOWFLAKE_PRIVATE_KEY_PATH=/path/to/your/snowflake_key.p8
# Private key content (PEM format with newlines) - used by test target and CI
# SNOWFLAKE_PRIVATE_KEY=

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:29092
KAFKA_TOPIC=stables-transfers
# Batch size for database inserts (default: 100)
KAFKA_BATCH_SIZE=100
# Maximum records to process before stopping (optional, unlimited if not set)
# KAFKA_MAX_RECORDS=10000

# GraphQL Indexer Configuration
# Replace with your actual HyperSync/indexer GraphQL endpoint
# Example: https://your-indexer.hypersync.xyz/graphql
GRAPHQL_ENDPOINT=http://localhost:8080/v1/graphql
POLL_INTERVAL=10

# Airflow Configuration
AIRFLOW_UID=50000
AIRFLOW_PORT=8080
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
# Optional: Additional pip packages to install in Airflow
# _PIP_ADDITIONAL_REQUIREMENTS=

# dbt Configuration (for Airflow)
DBT_PROJECT_DIR=/opt/airflow/dbt_project
DBT_PROFILES_DIR=/opt/airflow/dbt_project
